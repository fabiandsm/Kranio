{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c4ae39",
   "metadata": {},
   "source": [
    "# Construir pipeline ETL con manejo de errores completo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5d077",
   "metadata": {},
   "source": [
    "## Configurar logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133c0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('etl_ecommerce.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('etl_ecommerce')\n",
    "\n",
    "def log_etapa(etapa):\n",
    "    \"\"\"Decorator para logging de etapas\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            logger.info(f\"Iniciando {etapa}\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                duration = time.time() - start_time\n",
    "                logger.info(f\" {etapa} completada en {duration:.2f}s\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                duration = time.time() - start_time\n",
    "                logger.error(f\"{etapa} falló en {duration:.2f}s: {e}\")\n",
    "                raise e\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c5058",
   "metadata": {},
   "source": [
    "## Pipeline ETL con error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa311d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "class ETLPipeline:\n",
    "    def __init__(self):\n",
    "        self.logger = logger\n",
    "        self.errores = []\n",
    "    \n",
    "    @log_etapa(\"extracción de datos\")\n",
    "    def extract(self) -> pd.DataFrame:\n",
    "        \"\"\"Extraer datos con manejo de errores\"\"\"\n",
    "        try:\n",
    "            # Simular extracción (podría fallar)\n",
    "            if np.random.random() < 0.1:  # 10% chance de error\n",
    "                raise ConnectionError(\"Error de conexión a fuente de datos\")\n",
    "            \n",
    "            # Datos de ejemplo\n",
    "            datos = pd.DataFrame({\n",
    "                'orden_id': range(1, 101),\n",
    "                'cliente_id': np.random.randint(1, 21, 100),\n",
    "                'producto': np.random.choice(['A', 'B', 'C', 'D'], 100),\n",
    "                'cantidad': np.random.randint(1, 6, 100),\n",
    "                'precio': np.round(np.random.uniform(10, 200, 100), 2)\n",
    "            })\n",
    "            \n",
    "            self.logger.info(f\"Extraídos {len(datos)} registros\")\n",
    "            return datos\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.errores.append(f\"Extract: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    @log_etapa(\"transformación de datos\")\n",
    "    def transform(self, datos: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transformar datos con validaciones\"\"\"\n",
    "        try:\n",
    "            df = datos.copy()\n",
    "            \n",
    "            # Validar datos de entrada\n",
    "            if df.empty:\n",
    "                raise ValueError(\"No hay datos para transformar\")\n",
    "            \n",
    "            # Transformaciones\n",
    "            df['total'] = df['cantidad'] * df['precio']\n",
    "            df['categoria_precio'] = pd.cut(\n",
    "                df['precio'], \n",
    "                bins=[0, 50, 100, 200], \n",
    "                labels=['Bajo', 'Medio', 'Alto']\n",
    "            )\n",
    "            \n",
    "            # Validar transformaciones\n",
    "            if df['total'].isnull().any():\n",
    "                raise ValueError(\"Transformación produjo valores nulos\")\n",
    "            \n",
    "            self.logger.info(f\"Transformados {len(df)} registros\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.errores.append(f\"Transform: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    @log_etapa(\"carga de datos\")\n",
    "    def load(self, datos: pd.DataFrame) -> bool:\n",
    "        \"\"\"Cargar datos con verificación\"\"\"\n",
    "        try:\n",
    "            # Simular carga (podría fallar)\n",
    "            if np.random.random() < 0.05:  # 5% chance de error\n",
    "                raise Exception(\"Error de conexión a base de datos\")\n",
    "            \n",
    "            # En producción: datos.to_sql('ventas', engine, if_exists='append')\n",
    "            self.logger.info(f\"Cargados {len(datos)} registros exitosamente\")\n",
    "            \n",
    "            # Validar carga\n",
    "            registros_esperados = len(datos)\n",
    "            registros_cargados = len(datos)  # Simulado\n",
    "            \n",
    "            if registros_cargados != registros_esperados:\n",
    "                raise ValueError(f\"Carga incompleta: {registros_cargados}/{registros_esperados}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.errores.append(f\"Load: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def ejecutar_pipeline(self) -> Dict[str, Any]:\n",
    "        \"\"\"Ejecutar pipeline completo con manejo de errores\"\"\"\n",
    "        self.logger.info(\"Iniciando pipeline ETL completo\")\n",
    "        \n",
    "        try:\n",
    "            # Extract\n",
    "            datos_crudo = self.extract()\n",
    "            \n",
    "            # Transform\n",
    "            datos_transformados = self.transform(datos_crudo)\n",
    "            \n",
    "            # Load\n",
    "            exito = self.load(datos_transformados)\n",
    "            \n",
    "            resultado = {\n",
    "                'exito': True,\n",
    "                'registros_procesados': len(datos_transformados),\n",
    "                'errores': self.errores\n",
    "            }\n",
    "            \n",
    "            self.logger.info(\"Pipeline ETL completado exitosamente\")\n",
    "            return resultado\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline ETL falló: {e}\")\n",
    "            \n",
    "            return {\n",
    "                'exito': False,\n",
    "                'error_principal': str(e),\n",
    "                'errores': self.errores\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b732f5",
   "metadata": {},
   "source": [
    "## Ejecutar y validar pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09fe141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 04:23:23,677 - INFO - Iniciando pipeline ETL completo\n",
      "2026-01-16 04:23:23,677 - INFO - Iniciando extracción de datos\n",
      "2026-01-16 04:23:23,677 - INFO - Extraídos 100 registros\n",
      "2026-01-16 04:23:23,677 - INFO -  extracción de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,677 - INFO - Iniciando transformación de datos\n",
      "2026-01-16 04:23:23,677 - INFO - Transformados 100 registros\n",
      "2026-01-16 04:23:23,677 - INFO -  transformación de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,677 - INFO - Iniciando carga de datos\n",
      "2026-01-16 04:23:23,677 - INFO - Cargados 100 registros exitosamente\n",
      "2026-01-16 04:23:23,689 - INFO -  carga de datos completada en 0.01s\n",
      "2026-01-16 04:23:23,689 - INFO - Pipeline ETL completado exitosamente\n",
      "2026-01-16 04:23:23,689 - INFO - Iniciando pipeline ETL completo\n",
      "2026-01-16 04:23:23,689 - INFO - Iniciando extracción de datos\n",
      "2026-01-16 04:23:23,689 - INFO - Extraídos 100 registros\n",
      "2026-01-16 04:23:23,689 - INFO -  extracción de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,689 - INFO - Iniciando transformación de datos\n",
      "2026-01-16 04:23:23,695 - INFO - Transformados 100 registros\n",
      "2026-01-16 04:23:23,696 - INFO -  transformación de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,696 - INFO - Iniciando carga de datos\n",
      "2026-01-16 04:23:23,696 - INFO - Cargados 100 registros exitosamente\n",
      "2026-01-16 04:23:23,696 - INFO -  carga de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,696 - INFO - Pipeline ETL completado exitosamente\n",
      "2026-01-16 04:23:23,696 - INFO - Iniciando pipeline ETL completo\n",
      "2026-01-16 04:23:23,696 - INFO - Iniciando extracción de datos\n",
      "2026-01-16 04:23:23,696 - INFO - Extraídos 100 registros\n",
      "2026-01-16 04:23:23,696 - INFO -  extracción de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,696 - INFO - Iniciando transformación de datos\n",
      "2026-01-16 04:23:23,696 - INFO - Transformados 100 registros\n",
      "2026-01-16 04:23:23,696 - INFO -  transformación de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,696 - INFO - Iniciando carga de datos\n",
      "2026-01-16 04:23:23,696 - INFO - Cargados 100 registros exitosamente\n",
      "2026-01-16 04:23:23,696 - INFO -  carga de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,696 - INFO - Pipeline ETL completado exitosamente\n",
      "2026-01-16 04:23:23,696 - INFO - Iniciando pipeline ETL completo\n",
      "2026-01-16 04:23:23,696 - INFO - Iniciando extracción de datos\n",
      "2026-01-16 04:23:23,696 - INFO - Extraídos 100 registros\n",
      "2026-01-16 04:23:23,696 - INFO -  extracción de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,696 - INFO - Iniciando transformación de datos\n",
      "2026-01-16 04:23:23,696 - INFO - Transformados 100 registros\n",
      "2026-01-16 04:23:23,696 - INFO -  transformación de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,696 - INFO - Iniciando carga de datos\n",
      "2026-01-16 04:23:23,696 - INFO - Cargados 100 registros exitosamente\n",
      "2026-01-16 04:23:23,696 - INFO -  carga de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,710 - INFO - Pipeline ETL completado exitosamente\n",
      "2026-01-16 04:23:23,710 - INFO - Iniciando pipeline ETL completo\n",
      "2026-01-16 04:23:23,711 - INFO - Iniciando extracción de datos\n",
      "2026-01-16 04:23:23,711 - INFO - Extraídos 100 registros\n",
      "2026-01-16 04:23:23,711 - INFO -  extracción de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,711 - INFO - Iniciando transformación de datos\n",
      "2026-01-16 04:23:23,711 - INFO - Transformados 100 registros\n",
      "2026-01-16 04:23:23,711 - INFO -  transformación de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,711 - INFO - Iniciando carga de datos\n",
      "2026-01-16 04:23:23,711 - INFO - Cargados 100 registros exitosamente\n",
      "2026-01-16 04:23:23,711 - INFO -  carga de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,711 - INFO - Pipeline ETL completado exitosamente\n",
      "2026-01-16 04:23:23,711 - INFO - Iniciando pipeline ETL completo\n",
      "2026-01-16 04:23:23,711 - INFO - Iniciando extracción de datos\n",
      "2026-01-16 04:23:23,711 - INFO - Extraídos 100 registros\n",
      "2026-01-16 04:23:23,711 - INFO -  extracción de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,711 - INFO - Iniciando transformación de datos\n",
      "2026-01-16 04:23:23,711 - INFO - Transformados 100 registros\n",
      "2026-01-16 04:23:23,711 - INFO -  transformación de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,711 - INFO - Iniciando carga de datos\n",
      "2026-01-16 04:23:23,711 - INFO - Cargados 100 registros exitosamente\n",
      "2026-01-16 04:23:23,711 - INFO -  carga de datos completada en 0.00s\n",
      "2026-01-16 04:23:23,711 - INFO - Pipeline ETL completado exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultado del pipeline:\n",
      "Éxito: True\n",
      "Registros procesados: 100\n",
      "Errores registrados: 0\n",
      "\n",
      "--- Ejecución 1 ---\n",
      "\n",
      "--- Ejecución 2 ---\n",
      "\n",
      "--- Ejecución 3 ---\n",
      "\n",
      "--- Ejecución 4 ---\n",
      "\n",
      "--- Ejecución 5 ---\n",
      ".1%\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar pipeline con diferentes escenarios\n",
    "pipeline = ETLPipeline()\n",
    "\n",
    "# Ejecución exitosa\n",
    "resultado = pipeline.ejecutar_pipeline()\n",
    "\n",
    "print(f\"\\nResultado del pipeline:\")\n",
    "print(f\"Éxito: {resultado['exito']}\")\n",
    "if resultado['exito']:\n",
    "    print(f\"Registros procesados: {resultado['registros_procesados']}\")\n",
    "else:\n",
    "    print(f\"Error principal: {resultado['error_principal']}\")\n",
    "\n",
    "print(f\"Errores registrados: {len(resultado['errores'])}\")\n",
    "for error in resultado['errores']:\n",
    "    print(f\"  - {error}\")\n",
    "\n",
    "# Ejecutar múltiples veces para probar robustez\n",
    "resultados_multiples = []\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Ejecución {i+1} ---\")\n",
    "    pipeline_i = ETLPipeline()\n",
    "    resultado_i = pipeline_i.ejecutar_pipeline()\n",
    "    resultados_multiples.append(resultado_i['exito'])\n",
    "\n",
    "exito_rate = sum(resultados_multiples) / len(resultados_multiples)\n",
    "print(\".1%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a34595",
   "metadata": {},
   "source": [
    "## Verificación\n",
    "1️ ¿Qué información debería incluir en los logs para facilitar el debugging?\n",
    "-   Los logs deben registrar qué etapa se ejecuta, cuándo y con qué resultado. En el ejercicio, es clave incluir el nombre de la etapa (extract, transform, load), el timestamp, la duración de ejecución, la cantidad de registros procesados y el mensaje de error exacto cuando ocurre una falla. Además, guardar el tipo de excepción y el contexto del fallo permite identificar si el problema es de datos, conexión o lógica. Esta información permite reproducir el error, aislar la causa raíz y evaluar el impacto del fallo sin inspeccionar manualmente el código.\n",
    "\n",
    "2️ ¿Cómo decides entre continuar el pipeline con errores parciales vs detenerlo completamente?\n",
    "-   La decisión depende del impacto del error en la calidad e integridad de los datos. En el ejercicio, errores en extract o load justifican detener el pipeline, ya que comprometen la completitud o la persistencia de la información. En cambio, errores parciales y controlables en transformaciones no críticas pueden registrarse y permitir la continuidad, siempre que existan validaciones y trazabilidad. El criterio clave es si el resultado sigue siendo confiable para análisis o reporting; si no lo es, el pipeline debe fallar de forma explícita.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
